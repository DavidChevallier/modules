"""
This file was generated by the KCL auto-gen tool. DO NOT EDIT.
Editing this file might prove futile when you re-run the KCL auto-gen generate command.
"""
import k8s.apimachinery.pkg.apis.meta.v1


schema Job:
    """
    Job is the Schema for the Jobs API. Manages a job resource within a Dataproc cluster.

    Attributes
    ----------
    apiVersion : str, default is "dataproc.gcp.upbound.io/v1beta1", required
        APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources
    kind : str, default is "Job", required
        Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds
    metadata : v1.ObjectMeta, default is Undefined, optional
        metadata
    spec : DataprocGcpUpboundIoV1beta1JobSpec, default is Undefined, required
        spec
    status : DataprocGcpUpboundIoV1beta1JobStatus, default is Undefined, optional
        status
    """


    apiVersion: "dataproc.gcp.upbound.io/v1beta1" = "dataproc.gcp.upbound.io/v1beta1"

    kind: "Job" = "Job"

    metadata?: v1.ObjectMeta

    spec: DataprocGcpUpboundIoV1beta1JobSpec

    status?: DataprocGcpUpboundIoV1beta1JobStatus


schema DataprocGcpUpboundIoV1beta1JobSpec:
    """
    JobSpec defines the desired state of Job

    Attributes
    ----------
    deletionPolicy : str, default is "Delete", optional
        DeletionPolicy specifies what will happen to the underlying external
        when this managed resource is deleted - either "Delete" or "Orphan" the
        external resource.
        This field is planned to be deprecated in favor of the ManagementPolicies
        field in a future release. Currently, both could be set independently and
        non-default values would be honored if the feature flag is enabled.
        See the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223
    forProvider : DataprocGcpUpboundIoV1beta1JobSpecForProvider, default is Undefined, required
        for provider
    initProvider : DataprocGcpUpboundIoV1beta1JobSpecInitProvider, default is Undefined, optional
        init provider
    managementPolicies : [str], default is ["*"], optional
        THIS IS A BETA FIELD. It is on by default but can be opted out
        through a Crossplane feature flag.
        ManagementPolicies specify the array of actions Crossplane is allowed to
        take on the managed and external resources.
        This field is planned to replace the DeletionPolicy field in a future
        release. Currently, both could be set independently and non-default
        values would be honored if the feature flag is enabled. If both are
        custom, the DeletionPolicy field will be ignored.
        See the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223
        and this one: https://github.com/crossplane/crossplane/blob/444267e84783136daa93568b364a5f01228cacbe/design/one-pager-ignore-changes.md
    providerConfigRef : DataprocGcpUpboundIoV1beta1JobSpecProviderConfigRef, default is Undefined, optional
        provider config ref
    publishConnectionDetailsTo : DataprocGcpUpboundIoV1beta1JobSpecPublishConnectionDetailsTo, default is Undefined, optional
        publish connection details to
    writeConnectionSecretToRef : DataprocGcpUpboundIoV1beta1JobSpecWriteConnectionSecretToRef, default is Undefined, optional
        write connection secret to ref
    """


    deletionPolicy?: "Orphan" | "Delete" = "Delete"

    forProvider: DataprocGcpUpboundIoV1beta1JobSpecForProvider

    initProvider?: DataprocGcpUpboundIoV1beta1JobSpecInitProvider

    managementPolicies?: [str] = ["*"]

    providerConfigRef?: DataprocGcpUpboundIoV1beta1JobSpecProviderConfigRef

    publishConnectionDetailsTo?: DataprocGcpUpboundIoV1beta1JobSpecPublishConnectionDetailsTo

    writeConnectionSecretToRef?: DataprocGcpUpboundIoV1beta1JobSpecWriteConnectionSecretToRef


schema DataprocGcpUpboundIoV1beta1JobSpecForProvider:
    """
    dataproc gcp upbound io v1beta1 job spec for provider

    Attributes
    ----------
    forceDelete : bool, default is Undefined, optional
        By default, you can only delete inactive jobs within
        Dataproc. Setting this to true, and calling destroy, will ensure that the
        job is first cancelled before issuing the delete.
    hadoopConfig : [DataprocGcpUpboundIoV1beta1JobSpecForProviderHadoopConfigItems0], default is Undefined, optional
        hadoop config
    hiveConfig : [DataprocGcpUpboundIoV1beta1JobSpecForProviderHiveConfigItems0], default is Undefined, optional
        hive config
    labels : {str:str}, default is Undefined, optional
        The list of labels (key/value pairs) to add to the job.
        Note: This field is non-authoritative, and will only manage the labels present in your configuration.
        Please refer to the field 'effective_labels' for all of the labels present on the resource.
    pigConfig : [DataprocGcpUpboundIoV1beta1JobSpecForProviderPigConfigItems0], default is Undefined, optional
        pig config
    placement : [DataprocGcpUpboundIoV1beta1JobSpecForProviderPlacementItems0], default is Undefined, optional
        placement
    prestoConfig : [DataprocGcpUpboundIoV1beta1JobSpecForProviderPrestoConfigItems0], default is Undefined, optional
        presto config
    project : str, default is Undefined, optional
        The project in which the cluster can be found and jobs
        subsequently run against. If it is not provided, the provider project is used.
    pysparkConfig : [DataprocGcpUpboundIoV1beta1JobSpecForProviderPysparkConfigItems0], default is Undefined, optional
        pyspark config
    reference : [DataprocGcpUpboundIoV1beta1JobSpecForProviderReferenceItems0], default is Undefined, optional
        reference
    region : str, default is Undefined, optional
        The Cloud Dataproc region. This essentially determines which clusters are available
        for this job to be submitted to. If not specified, defaults to global.
    regionRef : DataprocGcpUpboundIoV1beta1JobSpecForProviderRegionRef, default is Undefined, optional
        region ref
    regionSelector : DataprocGcpUpboundIoV1beta1JobSpecForProviderRegionSelector, default is Undefined, optional
        region selector
    scheduling : [DataprocGcpUpboundIoV1beta1JobSpecForProviderSchedulingItems0], default is Undefined, optional
        scheduling
    sparkConfig : [DataprocGcpUpboundIoV1beta1JobSpecForProviderSparkConfigItems0], default is Undefined, optional
        spark config
    sparksqlConfig : [DataprocGcpUpboundIoV1beta1JobSpecForProviderSparksqlConfigItems0], default is Undefined, optional
        sparksql config
    """


    forceDelete?: bool

    hadoopConfig?: [DataprocGcpUpboundIoV1beta1JobSpecForProviderHadoopConfigItems0]

    hiveConfig?: [DataprocGcpUpboundIoV1beta1JobSpecForProviderHiveConfigItems0]

    labels?: {str:str}

    pigConfig?: [DataprocGcpUpboundIoV1beta1JobSpecForProviderPigConfigItems0]

    placement?: [DataprocGcpUpboundIoV1beta1JobSpecForProviderPlacementItems0]

    prestoConfig?: [DataprocGcpUpboundIoV1beta1JobSpecForProviderPrestoConfigItems0]

    project?: str

    pysparkConfig?: [DataprocGcpUpboundIoV1beta1JobSpecForProviderPysparkConfigItems0]

    reference?: [DataprocGcpUpboundIoV1beta1JobSpecForProviderReferenceItems0]

    region?: str

    regionRef?: DataprocGcpUpboundIoV1beta1JobSpecForProviderRegionRef

    regionSelector?: DataprocGcpUpboundIoV1beta1JobSpecForProviderRegionSelector

    scheduling?: [DataprocGcpUpboundIoV1beta1JobSpecForProviderSchedulingItems0]

    sparkConfig?: [DataprocGcpUpboundIoV1beta1JobSpecForProviderSparkConfigItems0]

    sparksqlConfig?: [DataprocGcpUpboundIoV1beta1JobSpecForProviderSparksqlConfigItems0]


schema DataprocGcpUpboundIoV1beta1JobSpecForProviderHadoopConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job spec for provider hadoop config items0

    Attributes
    ----------
    archiveUris : [str], default is Undefined, optional
        HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
    args : [str], default is Undefined, optional
        The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
    fileUris : [str], default is Undefined, optional
        HCFS URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
    jarFileUris : [str], default is Undefined, optional
        HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
    loggingConfig : [DataprocGcpUpboundIoV1beta1JobSpecForProviderHadoopConfigItems0LoggingConfigItems0], default is Undefined, optional
        logging config
    mainClass : str, default is Undefined, optional
        The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in jar_file_uris. Conflicts with main_jar_file_uri
    mainJarFileUri : str, default is Undefined, optional
        The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'. Conflicts with main_class
    properties : {str:str}, default is Undefined, optional
        A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code..
    """


    archiveUris?: [str]

    args?: [str]

    fileUris?: [str]

    jarFileUris?: [str]

    loggingConfig?: [DataprocGcpUpboundIoV1beta1JobSpecForProviderHadoopConfigItems0LoggingConfigItems0]

    mainClass?: str

    mainJarFileUri?: str

    properties?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobSpecForProviderHadoopConfigItems0LoggingConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job spec for provider hadoop config items0 logging config items0

    Attributes
    ----------
    driverLogLevels : {str:str}, default is Undefined, optional
        The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    """


    driverLogLevels?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobSpecForProviderHiveConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job spec for provider hive config items0

    Attributes
    ----------
    continueOnFailure : bool, default is Undefined, optional
        Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
    jarFileUris : [str], default is Undefined, optional
        HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
    properties : {str:str}, default is Undefined, optional
        A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code..
    queryFileUri : str, default is Undefined, optional
        HCFS URI of file containing Hive script to execute as the job.
        Conflicts with query_list
    queryList : [str], default is Undefined, optional
        The list of Hive queries or statements to execute as part of the job.
        Conflicts with query_file_uri
    scriptVariables : {str:str}, default is Undefined, optional
        Mapping of query variable names to values (equivalent to the Hive command: SET name="value";).
    """


    continueOnFailure?: bool

    jarFileUris?: [str]

    properties?: {str:str}

    queryFileUri?: str

    queryList?: [str]

    scriptVariables?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobSpecForProviderPigConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job spec for provider pig config items0

    Attributes
    ----------
    continueOnFailure : bool, default is Undefined, optional
        Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
    jarFileUris : [str], default is Undefined, optional
        HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
    loggingConfig : [DataprocGcpUpboundIoV1beta1JobSpecForProviderPigConfigItems0LoggingConfigItems0], default is Undefined, optional
        logging config
    properties : {str:str}, default is Undefined, optional
        A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
    queryFileUri : str, default is Undefined, optional
        HCFS URI of file containing Hive script to execute as the job.
        Conflicts with query_list
    queryList : [str], default is Undefined, optional
        The list of Hive queries or statements to execute as part of the job.
        Conflicts with query_file_uri
    scriptVariables : {str:str}, default is Undefined, optional
        Mapping of query variable names to values (equivalent to the Pig command: name=[value]).
    """


    continueOnFailure?: bool

    jarFileUris?: [str]

    loggingConfig?: [DataprocGcpUpboundIoV1beta1JobSpecForProviderPigConfigItems0LoggingConfigItems0]

    properties?: {str:str}

    queryFileUri?: str

    queryList?: [str]

    scriptVariables?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobSpecForProviderPigConfigItems0LoggingConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job spec for provider pig config items0 logging config items0

    Attributes
    ----------
    driverLogLevels : {str:str}, default is Undefined, optional
        The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    """


    driverLogLevels?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobSpecForProviderPlacementItems0:
    """
    dataproc gcp upbound io v1beta1 job spec for provider placement items0

    Attributes
    ----------
    clusterName : str, default is Undefined, optional
        The name of the cluster where the job
        will be submitted.
    clusterNameRef : DataprocGcpUpboundIoV1beta1JobSpecForProviderPlacementItems0ClusterNameRef, default is Undefined, optional
        cluster name ref
    clusterNameSelector : DataprocGcpUpboundIoV1beta1JobSpecForProviderPlacementItems0ClusterNameSelector, default is Undefined, optional
        cluster name selector
    """


    clusterName?: str

    clusterNameRef?: DataprocGcpUpboundIoV1beta1JobSpecForProviderPlacementItems0ClusterNameRef

    clusterNameSelector?: DataprocGcpUpboundIoV1beta1JobSpecForProviderPlacementItems0ClusterNameSelector


schema DataprocGcpUpboundIoV1beta1JobSpecForProviderPlacementItems0ClusterNameRef:
    """
    Reference to a Cluster in dataproc to populate clusterName.

    Attributes
    ----------
    name : str, default is Undefined, required
        Name of the referenced object.
    policy : DataprocGcpUpboundIoV1beta1JobSpecForProviderPlacementItems0ClusterNameRefPolicy, default is Undefined, optional
        policy
    """


    name: str

    policy?: DataprocGcpUpboundIoV1beta1JobSpecForProviderPlacementItems0ClusterNameRefPolicy


schema DataprocGcpUpboundIoV1beta1JobSpecForProviderPlacementItems0ClusterNameRefPolicy:
    """
    Policies for referencing.

    Attributes
    ----------
    resolution : str, default is "Required", optional
        Resolution specifies whether resolution of this reference is required.
        The default is 'Required', which means the reconcile will fail if the
        reference cannot be resolved. 'Optional' means this reference will be
        a no-op if it cannot be resolved.
    resolve : str, default is Undefined, optional
        Resolve specifies when this reference should be resolved. The default
        is 'IfNotPresent', which will attempt to resolve the reference only when
        the corresponding field is not present. Use 'Always' to resolve the
        reference on every reconcile.
    """


    resolution?: "Required" | "Optional" = "Required"

    resolve?: "Always" | "IfNotPresent"


schema DataprocGcpUpboundIoV1beta1JobSpecForProviderPlacementItems0ClusterNameSelector:
    """
    Selector for a Cluster in dataproc to populate clusterName.

    Attributes
    ----------
    matchControllerRef : bool, default is Undefined, optional
        MatchControllerRef ensures an object with the same controller reference
        as the selecting object is selected.
    matchLabels : {str:str}, default is Undefined, optional
        MatchLabels ensures an object with matching labels is selected.
    policy : DataprocGcpUpboundIoV1beta1JobSpecForProviderPlacementItems0ClusterNameSelectorPolicy, default is Undefined, optional
        policy
    """


    matchControllerRef?: bool

    matchLabels?: {str:str}

    policy?: DataprocGcpUpboundIoV1beta1JobSpecForProviderPlacementItems0ClusterNameSelectorPolicy


schema DataprocGcpUpboundIoV1beta1JobSpecForProviderPlacementItems0ClusterNameSelectorPolicy:
    """
    Policies for selection.

    Attributes
    ----------
    resolution : str, default is "Required", optional
        Resolution specifies whether resolution of this reference is required.
        The default is 'Required', which means the reconcile will fail if the
        reference cannot be resolved. 'Optional' means this reference will be
        a no-op if it cannot be resolved.
    resolve : str, default is Undefined, optional
        Resolve specifies when this reference should be resolved. The default
        is 'IfNotPresent', which will attempt to resolve the reference only when
        the corresponding field is not present. Use 'Always' to resolve the
        reference on every reconcile.
    """


    resolution?: "Required" | "Optional" = "Required"

    resolve?: "Always" | "IfNotPresent"


schema DataprocGcpUpboundIoV1beta1JobSpecForProviderPrestoConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job spec for provider presto config items0

    Attributes
    ----------
    clientTags : [str], default is Undefined, optional
        Presto client tags to attach to this query.
    continueOnFailure : bool, default is Undefined, optional
        Whether to continue executing queries if a query fails. Setting to true can be useful when executing independent parallel queries. Defaults to false.
    loggingConfig : [DataprocGcpUpboundIoV1beta1JobSpecForProviderPrestoConfigItems0LoggingConfigItems0], default is Undefined, optional
        logging config
    outputFormat : str, default is Undefined, optional
        The format in which query output will be displayed. See the Presto documentation for supported output formats.
    properties : {str:str}, default is Undefined, optional
        A mapping of property names to values. Used to set Presto session properties Equivalent to using the --session flag in the Presto CLI.
    queryFileUri : str, default is Undefined, optional
        The HCFS URI of the script that contains SQL queries.
        Conflicts with query_list
    queryList : [str], default is Undefined, optional
        The list of SQL queries or statements to execute as part of the job.
        Conflicts with query_file_uri
    """


    clientTags?: [str]

    continueOnFailure?: bool

    loggingConfig?: [DataprocGcpUpboundIoV1beta1JobSpecForProviderPrestoConfigItems0LoggingConfigItems0]

    outputFormat?: str

    properties?: {str:str}

    queryFileUri?: str

    queryList?: [str]


schema DataprocGcpUpboundIoV1beta1JobSpecForProviderPrestoConfigItems0LoggingConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job spec for provider presto config items0 logging config items0

    Attributes
    ----------
    driverLogLevels : {str:str}, default is Undefined, optional
        The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    """


    driverLogLevels?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobSpecForProviderPysparkConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job spec for provider pyspark config items0

    Attributes
    ----------
    archiveUris : [str], default is Undefined, optional
        HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
    args : [str], default is Undefined, optional
        The arguments to pass to the driver.
    fileUris : [str], default is Undefined, optional
        HCFS URIs of files to be copied to the working directory of Python drivers and distributed tasks. Useful for naively parallel tasks.
    jarFileUris : [str], default is Undefined, optional
        HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
    loggingConfig : [DataprocGcpUpboundIoV1beta1JobSpecForProviderPysparkConfigItems0LoggingConfigItems0], default is Undefined, optional
        logging config
    mainPythonFileUri : str, default is Undefined, optional
        The HCFS URI of the main Python file to use as the driver. Must be a .py file.
    properties : {str:str}, default is Undefined, optional
        A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
    pythonFileUris : [str], default is Undefined, optional
        HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
    """


    archiveUris?: [str]

    args?: [str]

    fileUris?: [str]

    jarFileUris?: [str]

    loggingConfig?: [DataprocGcpUpboundIoV1beta1JobSpecForProviderPysparkConfigItems0LoggingConfigItems0]

    mainPythonFileUri?: str

    properties?: {str:str}

    pythonFileUris?: [str]


schema DataprocGcpUpboundIoV1beta1JobSpecForProviderPysparkConfigItems0LoggingConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job spec for provider pyspark config items0 logging config items0

    Attributes
    ----------
    driverLogLevels : {str:str}, default is Undefined, optional
        The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    """


    driverLogLevels?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobSpecForProviderReferenceItems0:
    """
    dataproc gcp upbound io v1beta1 job spec for provider reference items0

    Attributes
    ----------
    jobId : str, default is Undefined, optional
        job Id
    """


    jobId?: str


schema DataprocGcpUpboundIoV1beta1JobSpecForProviderRegionRef:
    """
    Reference to a Cluster in dataproc to populate region.

    Attributes
    ----------
    name : str, default is Undefined, required
        Name of the referenced object.
    policy : DataprocGcpUpboundIoV1beta1JobSpecForProviderRegionRefPolicy, default is Undefined, optional
        policy
    """


    name: str

    policy?: DataprocGcpUpboundIoV1beta1JobSpecForProviderRegionRefPolicy


schema DataprocGcpUpboundIoV1beta1JobSpecForProviderRegionRefPolicy:
    """
    Policies for referencing.

    Attributes
    ----------
    resolution : str, default is "Required", optional
        Resolution specifies whether resolution of this reference is required.
        The default is 'Required', which means the reconcile will fail if the
        reference cannot be resolved. 'Optional' means this reference will be
        a no-op if it cannot be resolved.
    resolve : str, default is Undefined, optional
        Resolve specifies when this reference should be resolved. The default
        is 'IfNotPresent', which will attempt to resolve the reference only when
        the corresponding field is not present. Use 'Always' to resolve the
        reference on every reconcile.
    """


    resolution?: "Required" | "Optional" = "Required"

    resolve?: "Always" | "IfNotPresent"


schema DataprocGcpUpboundIoV1beta1JobSpecForProviderRegionSelector:
    """
    Selector for a Cluster in dataproc to populate region.

    Attributes
    ----------
    matchControllerRef : bool, default is Undefined, optional
        MatchControllerRef ensures an object with the same controller reference
        as the selecting object is selected.
    matchLabels : {str:str}, default is Undefined, optional
        MatchLabels ensures an object with matching labels is selected.
    policy : DataprocGcpUpboundIoV1beta1JobSpecForProviderRegionSelectorPolicy, default is Undefined, optional
        policy
    """


    matchControllerRef?: bool

    matchLabels?: {str:str}

    policy?: DataprocGcpUpboundIoV1beta1JobSpecForProviderRegionSelectorPolicy


schema DataprocGcpUpboundIoV1beta1JobSpecForProviderRegionSelectorPolicy:
    """
    Policies for selection.

    Attributes
    ----------
    resolution : str, default is "Required", optional
        Resolution specifies whether resolution of this reference is required.
        The default is 'Required', which means the reconcile will fail if the
        reference cannot be resolved. 'Optional' means this reference will be
        a no-op if it cannot be resolved.
    resolve : str, default is Undefined, optional
        Resolve specifies when this reference should be resolved. The default
        is 'IfNotPresent', which will attempt to resolve the reference only when
        the corresponding field is not present. Use 'Always' to resolve the
        reference on every reconcile.
    """


    resolution?: "Required" | "Optional" = "Required"

    resolve?: "Always" | "IfNotPresent"


schema DataprocGcpUpboundIoV1beta1JobSpecForProviderSchedulingItems0:
    """
    dataproc gcp upbound io v1beta1 job spec for provider scheduling items0

    Attributes
    ----------
    maxFailuresPerHour : float, default is Undefined, optional
        Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
    maxFailuresTotal : float, default is Undefined, optional
        Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
    """


    maxFailuresPerHour?: float

    maxFailuresTotal?: float


schema DataprocGcpUpboundIoV1beta1JobSpecForProviderSparkConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job spec for provider spark config items0

    Attributes
    ----------
    archiveUris : [str], default is Undefined, optional
        HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
    args : [str], default is Undefined, optional
        The arguments to pass to the driver.
    fileUris : [str], default is Undefined, optional
        HCFS URIs of files to be copied to the working directory of Spark drivers and distributed tasks. Useful for naively parallel tasks.
    jarFileUris : [str], default is Undefined, optional
        HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
    loggingConfig : [DataprocGcpUpboundIoV1beta1JobSpecForProviderSparkConfigItems0LoggingConfigItems0], default is Undefined, optional
        logging config
    mainClass : str, default is Undefined, optional
        The class containing the main method of the driver. Must be in a
        provided jar or jar that is already on the classpath. Conflicts with main_jar_file_uri
    mainJarFileUri : str, default is Undefined, optional
        The HCFS URI of jar file containing
        the driver jar. Conflicts with main_class
    properties : {str:str}, default is Undefined, optional
        A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
    """


    archiveUris?: [str]

    args?: [str]

    fileUris?: [str]

    jarFileUris?: [str]

    loggingConfig?: [DataprocGcpUpboundIoV1beta1JobSpecForProviderSparkConfigItems0LoggingConfigItems0]

    mainClass?: str

    mainJarFileUri?: str

    properties?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobSpecForProviderSparkConfigItems0LoggingConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job spec for provider spark config items0 logging config items0

    Attributes
    ----------
    driverLogLevels : {str:str}, default is Undefined, optional
        The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    """


    driverLogLevels?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobSpecForProviderSparksqlConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job spec for provider sparksql config items0

    Attributes
    ----------
    jarFileUris : [str], default is Undefined, optional
        HCFS URIs of jar files to be added to the Spark CLASSPATH.
    loggingConfig : [DataprocGcpUpboundIoV1beta1JobSpecForProviderSparksqlConfigItems0LoggingConfigItems0], default is Undefined, optional
        logging config
    properties : {str:str}, default is Undefined, optional
        A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
    queryFileUri : str, default is Undefined, optional
        The HCFS URI of the script that contains SQL queries.
        Conflicts with query_list
    queryList : [str], default is Undefined, optional
        The list of SQL queries or statements to execute as part of the job.
        Conflicts with query_file_uri
    scriptVariables : {str:str}, default is Undefined, optional
        Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
    """


    jarFileUris?: [str]

    loggingConfig?: [DataprocGcpUpboundIoV1beta1JobSpecForProviderSparksqlConfigItems0LoggingConfigItems0]

    properties?: {str:str}

    queryFileUri?: str

    queryList?: [str]

    scriptVariables?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobSpecForProviderSparksqlConfigItems0LoggingConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job spec for provider sparksql config items0 logging config items0

    Attributes
    ----------
    driverLogLevels : {str:str}, default is Undefined, optional
        The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    """


    driverLogLevels?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobSpecInitProvider:
    """
    THIS IS A BETA FIELD. It will be honored
    unless the Management Policies feature flag is disabled.
    InitProvider holds the same fields as ForProvider, with the exception
    of Identifier and other resource reference fields. The fields that are
    in InitProvider are merged into ForProvider when the resource is created.
    The same fields are also added to the terraform ignore_changes hook, to
    avoid updating them after creation. This is useful for fields that are
    required on creation, but we do not desire to update them after creation,
    for example because of an external controller is managing them, like an
    autoscaler.

    Attributes
    ----------
    forceDelete : bool, default is Undefined, optional
        By default, you can only delete inactive jobs within
        Dataproc. Setting this to true, and calling destroy, will ensure that the
        job is first cancelled before issuing the delete.
    hadoopConfig : [DataprocGcpUpboundIoV1beta1JobSpecInitProviderHadoopConfigItems0], default is Undefined, optional
        hadoop config
    hiveConfig : [DataprocGcpUpboundIoV1beta1JobSpecInitProviderHiveConfigItems0], default is Undefined, optional
        hive config
    labels : {str:str}, default is Undefined, optional
        The list of labels (key/value pairs) to add to the job.
        Note: This field is non-authoritative, and will only manage the labels present in your configuration.
        Please refer to the field 'effective_labels' for all of the labels present on the resource.
    pigConfig : [DataprocGcpUpboundIoV1beta1JobSpecInitProviderPigConfigItems0], default is Undefined, optional
        pig config
    placement : [DataprocGcpUpboundIoV1beta1JobSpecInitProviderPlacementItems0], default is Undefined, optional
        placement
    prestoConfig : [DataprocGcpUpboundIoV1beta1JobSpecInitProviderPrestoConfigItems0], default is Undefined, optional
        presto config
    project : str, default is Undefined, optional
        The project in which the cluster can be found and jobs
        subsequently run against. If it is not provided, the provider project is used.
    pysparkConfig : [DataprocGcpUpboundIoV1beta1JobSpecInitProviderPysparkConfigItems0], default is Undefined, optional
        pyspark config
    reference : [DataprocGcpUpboundIoV1beta1JobSpecInitProviderReferenceItems0], default is Undefined, optional
        reference
    region : str, default is Undefined, optional
        The Cloud Dataproc region. This essentially determines which clusters are available
        for this job to be submitted to. If not specified, defaults to global.
    regionRef : DataprocGcpUpboundIoV1beta1JobSpecInitProviderRegionRef, default is Undefined, optional
        region ref
    regionSelector : DataprocGcpUpboundIoV1beta1JobSpecInitProviderRegionSelector, default is Undefined, optional
        region selector
    scheduling : [DataprocGcpUpboundIoV1beta1JobSpecInitProviderSchedulingItems0], default is Undefined, optional
        scheduling
    sparkConfig : [DataprocGcpUpboundIoV1beta1JobSpecInitProviderSparkConfigItems0], default is Undefined, optional
        spark config
    sparksqlConfig : [DataprocGcpUpboundIoV1beta1JobSpecInitProviderSparksqlConfigItems0], default is Undefined, optional
        sparksql config
    """


    forceDelete?: bool

    hadoopConfig?: [DataprocGcpUpboundIoV1beta1JobSpecInitProviderHadoopConfigItems0]

    hiveConfig?: [DataprocGcpUpboundIoV1beta1JobSpecInitProviderHiveConfigItems0]

    labels?: {str:str}

    pigConfig?: [DataprocGcpUpboundIoV1beta1JobSpecInitProviderPigConfigItems0]

    placement?: [DataprocGcpUpboundIoV1beta1JobSpecInitProviderPlacementItems0]

    prestoConfig?: [DataprocGcpUpboundIoV1beta1JobSpecInitProviderPrestoConfigItems0]

    project?: str

    pysparkConfig?: [DataprocGcpUpboundIoV1beta1JobSpecInitProviderPysparkConfigItems0]

    reference?: [DataprocGcpUpboundIoV1beta1JobSpecInitProviderReferenceItems0]

    region?: str

    regionRef?: DataprocGcpUpboundIoV1beta1JobSpecInitProviderRegionRef

    regionSelector?: DataprocGcpUpboundIoV1beta1JobSpecInitProviderRegionSelector

    scheduling?: [DataprocGcpUpboundIoV1beta1JobSpecInitProviderSchedulingItems0]

    sparkConfig?: [DataprocGcpUpboundIoV1beta1JobSpecInitProviderSparkConfigItems0]

    sparksqlConfig?: [DataprocGcpUpboundIoV1beta1JobSpecInitProviderSparksqlConfigItems0]


schema DataprocGcpUpboundIoV1beta1JobSpecInitProviderHadoopConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job spec init provider hadoop config items0

    Attributes
    ----------
    archiveUris : [str], default is Undefined, optional
        HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
    args : [str], default is Undefined, optional
        The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
    fileUris : [str], default is Undefined, optional
        HCFS URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
    jarFileUris : [str], default is Undefined, optional
        HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
    loggingConfig : [DataprocGcpUpboundIoV1beta1JobSpecInitProviderHadoopConfigItems0LoggingConfigItems0], default is Undefined, optional
        logging config
    mainClass : str, default is Undefined, optional
        The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in jar_file_uris. Conflicts with main_jar_file_uri
    mainJarFileUri : str, default is Undefined, optional
        The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'. Conflicts with main_class
    properties : {str:str}, default is Undefined, optional
        A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code..
    """


    archiveUris?: [str]

    args?: [str]

    fileUris?: [str]

    jarFileUris?: [str]

    loggingConfig?: [DataprocGcpUpboundIoV1beta1JobSpecInitProviderHadoopConfigItems0LoggingConfigItems0]

    mainClass?: str

    mainJarFileUri?: str

    properties?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobSpecInitProviderHadoopConfigItems0LoggingConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job spec init provider hadoop config items0 logging config items0

    Attributes
    ----------
    driverLogLevels : {str:str}, default is Undefined, optional
        The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    """


    driverLogLevels?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobSpecInitProviderHiveConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job spec init provider hive config items0

    Attributes
    ----------
    continueOnFailure : bool, default is Undefined, optional
        Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
    jarFileUris : [str], default is Undefined, optional
        HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
    properties : {str:str}, default is Undefined, optional
        A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code..
    queryFileUri : str, default is Undefined, optional
        HCFS URI of file containing Hive script to execute as the job.
        Conflicts with query_list
    queryList : [str], default is Undefined, optional
        The list of Hive queries or statements to execute as part of the job.
        Conflicts with query_file_uri
    scriptVariables : {str:str}, default is Undefined, optional
        Mapping of query variable names to values (equivalent to the Hive command: SET name="value";).
    """


    continueOnFailure?: bool

    jarFileUris?: [str]

    properties?: {str:str}

    queryFileUri?: str

    queryList?: [str]

    scriptVariables?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobSpecInitProviderPigConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job spec init provider pig config items0

    Attributes
    ----------
    continueOnFailure : bool, default is Undefined, optional
        Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
    jarFileUris : [str], default is Undefined, optional
        HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
    loggingConfig : [DataprocGcpUpboundIoV1beta1JobSpecInitProviderPigConfigItems0LoggingConfigItems0], default is Undefined, optional
        logging config
    properties : {str:str}, default is Undefined, optional
        A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
    queryFileUri : str, default is Undefined, optional
        HCFS URI of file containing Hive script to execute as the job.
        Conflicts with query_list
    queryList : [str], default is Undefined, optional
        The list of Hive queries or statements to execute as part of the job.
        Conflicts with query_file_uri
    scriptVariables : {str:str}, default is Undefined, optional
        Mapping of query variable names to values (equivalent to the Pig command: name=[value]).
    """


    continueOnFailure?: bool

    jarFileUris?: [str]

    loggingConfig?: [DataprocGcpUpboundIoV1beta1JobSpecInitProviderPigConfigItems0LoggingConfigItems0]

    properties?: {str:str}

    queryFileUri?: str

    queryList?: [str]

    scriptVariables?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobSpecInitProviderPigConfigItems0LoggingConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job spec init provider pig config items0 logging config items0

    Attributes
    ----------
    driverLogLevels : {str:str}, default is Undefined, optional
        The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    """


    driverLogLevels?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobSpecInitProviderPlacementItems0:
    """
    dataproc gcp upbound io v1beta1 job spec init provider placement items0

    Attributes
    ----------
    clusterName : str, default is Undefined, optional
        The name of the cluster where the job
        will be submitted.
    clusterNameRef : DataprocGcpUpboundIoV1beta1JobSpecInitProviderPlacementItems0ClusterNameRef, default is Undefined, optional
        cluster name ref
    clusterNameSelector : DataprocGcpUpboundIoV1beta1JobSpecInitProviderPlacementItems0ClusterNameSelector, default is Undefined, optional
        cluster name selector
    """


    clusterName?: str

    clusterNameRef?: DataprocGcpUpboundIoV1beta1JobSpecInitProviderPlacementItems0ClusterNameRef

    clusterNameSelector?: DataprocGcpUpboundIoV1beta1JobSpecInitProviderPlacementItems0ClusterNameSelector


schema DataprocGcpUpboundIoV1beta1JobSpecInitProviderPlacementItems0ClusterNameRef:
    """
    Reference to a Cluster in dataproc to populate clusterName.

    Attributes
    ----------
    name : str, default is Undefined, required
        Name of the referenced object.
    policy : DataprocGcpUpboundIoV1beta1JobSpecInitProviderPlacementItems0ClusterNameRefPolicy, default is Undefined, optional
        policy
    """


    name: str

    policy?: DataprocGcpUpboundIoV1beta1JobSpecInitProviderPlacementItems0ClusterNameRefPolicy


schema DataprocGcpUpboundIoV1beta1JobSpecInitProviderPlacementItems0ClusterNameRefPolicy:
    """
    Policies for referencing.

    Attributes
    ----------
    resolution : str, default is "Required", optional
        Resolution specifies whether resolution of this reference is required.
        The default is 'Required', which means the reconcile will fail if the
        reference cannot be resolved. 'Optional' means this reference will be
        a no-op if it cannot be resolved.
    resolve : str, default is Undefined, optional
        Resolve specifies when this reference should be resolved. The default
        is 'IfNotPresent', which will attempt to resolve the reference only when
        the corresponding field is not present. Use 'Always' to resolve the
        reference on every reconcile.
    """


    resolution?: "Required" | "Optional" = "Required"

    resolve?: "Always" | "IfNotPresent"


schema DataprocGcpUpboundIoV1beta1JobSpecInitProviderPlacementItems0ClusterNameSelector:
    """
    Selector for a Cluster in dataproc to populate clusterName.

    Attributes
    ----------
    matchControllerRef : bool, default is Undefined, optional
        MatchControllerRef ensures an object with the same controller reference
        as the selecting object is selected.
    matchLabels : {str:str}, default is Undefined, optional
        MatchLabels ensures an object with matching labels is selected.
    policy : DataprocGcpUpboundIoV1beta1JobSpecInitProviderPlacementItems0ClusterNameSelectorPolicy, default is Undefined, optional
        policy
    """


    matchControllerRef?: bool

    matchLabels?: {str:str}

    policy?: DataprocGcpUpboundIoV1beta1JobSpecInitProviderPlacementItems0ClusterNameSelectorPolicy


schema DataprocGcpUpboundIoV1beta1JobSpecInitProviderPlacementItems0ClusterNameSelectorPolicy:
    """
    Policies for selection.

    Attributes
    ----------
    resolution : str, default is "Required", optional
        Resolution specifies whether resolution of this reference is required.
        The default is 'Required', which means the reconcile will fail if the
        reference cannot be resolved. 'Optional' means this reference will be
        a no-op if it cannot be resolved.
    resolve : str, default is Undefined, optional
        Resolve specifies when this reference should be resolved. The default
        is 'IfNotPresent', which will attempt to resolve the reference only when
        the corresponding field is not present. Use 'Always' to resolve the
        reference on every reconcile.
    """


    resolution?: "Required" | "Optional" = "Required"

    resolve?: "Always" | "IfNotPresent"


schema DataprocGcpUpboundIoV1beta1JobSpecInitProviderPrestoConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job spec init provider presto config items0

    Attributes
    ----------
    clientTags : [str], default is Undefined, optional
        Presto client tags to attach to this query.
    continueOnFailure : bool, default is Undefined, optional
        Whether to continue executing queries if a query fails. Setting to true can be useful when executing independent parallel queries. Defaults to false.
    loggingConfig : [DataprocGcpUpboundIoV1beta1JobSpecInitProviderPrestoConfigItems0LoggingConfigItems0], default is Undefined, optional
        logging config
    outputFormat : str, default is Undefined, optional
        The format in which query output will be displayed. See the Presto documentation for supported output formats.
    properties : {str:str}, default is Undefined, optional
        A mapping of property names to values. Used to set Presto session properties Equivalent to using the --session flag in the Presto CLI.
    queryFileUri : str, default is Undefined, optional
        The HCFS URI of the script that contains SQL queries.
        Conflicts with query_list
    queryList : [str], default is Undefined, optional
        The list of SQL queries or statements to execute as part of the job.
        Conflicts with query_file_uri
    """


    clientTags?: [str]

    continueOnFailure?: bool

    loggingConfig?: [DataprocGcpUpboundIoV1beta1JobSpecInitProviderPrestoConfigItems0LoggingConfigItems0]

    outputFormat?: str

    properties?: {str:str}

    queryFileUri?: str

    queryList?: [str]


schema DataprocGcpUpboundIoV1beta1JobSpecInitProviderPrestoConfigItems0LoggingConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job spec init provider presto config items0 logging config items0

    Attributes
    ----------
    driverLogLevels : {str:str}, default is Undefined, optional
        The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    """


    driverLogLevels?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobSpecInitProviderPysparkConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job spec init provider pyspark config items0

    Attributes
    ----------
    archiveUris : [str], default is Undefined, optional
        HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
    args : [str], default is Undefined, optional
        The arguments to pass to the driver.
    fileUris : [str], default is Undefined, optional
        HCFS URIs of files to be copied to the working directory of Python drivers and distributed tasks. Useful for naively parallel tasks.
    jarFileUris : [str], default is Undefined, optional
        HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
    loggingConfig : [DataprocGcpUpboundIoV1beta1JobSpecInitProviderPysparkConfigItems0LoggingConfigItems0], default is Undefined, optional
        logging config
    mainPythonFileUri : str, default is Undefined, optional
        The HCFS URI of the main Python file to use as the driver. Must be a .py file.
    properties : {str:str}, default is Undefined, optional
        A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
    pythonFileUris : [str], default is Undefined, optional
        HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
    """


    archiveUris?: [str]

    args?: [str]

    fileUris?: [str]

    jarFileUris?: [str]

    loggingConfig?: [DataprocGcpUpboundIoV1beta1JobSpecInitProviderPysparkConfigItems0LoggingConfigItems0]

    mainPythonFileUri?: str

    properties?: {str:str}

    pythonFileUris?: [str]


schema DataprocGcpUpboundIoV1beta1JobSpecInitProviderPysparkConfigItems0LoggingConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job spec init provider pyspark config items0 logging config items0

    Attributes
    ----------
    driverLogLevels : {str:str}, default is Undefined, optional
        The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    """


    driverLogLevels?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobSpecInitProviderReferenceItems0:
    """
    dataproc gcp upbound io v1beta1 job spec init provider reference items0

    Attributes
    ----------
    jobId : str, default is Undefined, optional
        job Id
    """


    jobId?: str


schema DataprocGcpUpboundIoV1beta1JobSpecInitProviderRegionRef:
    """
    Reference to a Cluster in dataproc to populate region.

    Attributes
    ----------
    name : str, default is Undefined, required
        Name of the referenced object.
    policy : DataprocGcpUpboundIoV1beta1JobSpecInitProviderRegionRefPolicy, default is Undefined, optional
        policy
    """


    name: str

    policy?: DataprocGcpUpboundIoV1beta1JobSpecInitProviderRegionRefPolicy


schema DataprocGcpUpboundIoV1beta1JobSpecInitProviderRegionRefPolicy:
    """
    Policies for referencing.

    Attributes
    ----------
    resolution : str, default is "Required", optional
        Resolution specifies whether resolution of this reference is required.
        The default is 'Required', which means the reconcile will fail if the
        reference cannot be resolved. 'Optional' means this reference will be
        a no-op if it cannot be resolved.
    resolve : str, default is Undefined, optional
        Resolve specifies when this reference should be resolved. The default
        is 'IfNotPresent', which will attempt to resolve the reference only when
        the corresponding field is not present. Use 'Always' to resolve the
        reference on every reconcile.
    """


    resolution?: "Required" | "Optional" = "Required"

    resolve?: "Always" | "IfNotPresent"


schema DataprocGcpUpboundIoV1beta1JobSpecInitProviderRegionSelector:
    """
    Selector for a Cluster in dataproc to populate region.

    Attributes
    ----------
    matchControllerRef : bool, default is Undefined, optional
        MatchControllerRef ensures an object with the same controller reference
        as the selecting object is selected.
    matchLabels : {str:str}, default is Undefined, optional
        MatchLabels ensures an object with matching labels is selected.
    policy : DataprocGcpUpboundIoV1beta1JobSpecInitProviderRegionSelectorPolicy, default is Undefined, optional
        policy
    """


    matchControllerRef?: bool

    matchLabels?: {str:str}

    policy?: DataprocGcpUpboundIoV1beta1JobSpecInitProviderRegionSelectorPolicy


schema DataprocGcpUpboundIoV1beta1JobSpecInitProviderRegionSelectorPolicy:
    """
    Policies for selection.

    Attributes
    ----------
    resolution : str, default is "Required", optional
        Resolution specifies whether resolution of this reference is required.
        The default is 'Required', which means the reconcile will fail if the
        reference cannot be resolved. 'Optional' means this reference will be
        a no-op if it cannot be resolved.
    resolve : str, default is Undefined, optional
        Resolve specifies when this reference should be resolved. The default
        is 'IfNotPresent', which will attempt to resolve the reference only when
        the corresponding field is not present. Use 'Always' to resolve the
        reference on every reconcile.
    """


    resolution?: "Required" | "Optional" = "Required"

    resolve?: "Always" | "IfNotPresent"


schema DataprocGcpUpboundIoV1beta1JobSpecInitProviderSchedulingItems0:
    """
    dataproc gcp upbound io v1beta1 job spec init provider scheduling items0

    Attributes
    ----------
    maxFailuresPerHour : float, default is Undefined, optional
        Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
    maxFailuresTotal : float, default is Undefined, optional
        Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
    """


    maxFailuresPerHour?: float

    maxFailuresTotal?: float


schema DataprocGcpUpboundIoV1beta1JobSpecInitProviderSparkConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job spec init provider spark config items0

    Attributes
    ----------
    archiveUris : [str], default is Undefined, optional
        HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
    args : [str], default is Undefined, optional
        The arguments to pass to the driver.
    fileUris : [str], default is Undefined, optional
        HCFS URIs of files to be copied to the working directory of Spark drivers and distributed tasks. Useful for naively parallel tasks.
    jarFileUris : [str], default is Undefined, optional
        HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
    loggingConfig : [DataprocGcpUpboundIoV1beta1JobSpecInitProviderSparkConfigItems0LoggingConfigItems0], default is Undefined, optional
        logging config
    mainClass : str, default is Undefined, optional
        The class containing the main method of the driver. Must be in a
        provided jar or jar that is already on the classpath. Conflicts with main_jar_file_uri
    mainJarFileUri : str, default is Undefined, optional
        The HCFS URI of jar file containing
        the driver jar. Conflicts with main_class
    properties : {str:str}, default is Undefined, optional
        A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
    """


    archiveUris?: [str]

    args?: [str]

    fileUris?: [str]

    jarFileUris?: [str]

    loggingConfig?: [DataprocGcpUpboundIoV1beta1JobSpecInitProviderSparkConfigItems0LoggingConfigItems0]

    mainClass?: str

    mainJarFileUri?: str

    properties?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobSpecInitProviderSparkConfigItems0LoggingConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job spec init provider spark config items0 logging config items0

    Attributes
    ----------
    driverLogLevels : {str:str}, default is Undefined, optional
        The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    """


    driverLogLevels?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobSpecInitProviderSparksqlConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job spec init provider sparksql config items0

    Attributes
    ----------
    jarFileUris : [str], default is Undefined, optional
        HCFS URIs of jar files to be added to the Spark CLASSPATH.
    loggingConfig : [DataprocGcpUpboundIoV1beta1JobSpecInitProviderSparksqlConfigItems0LoggingConfigItems0], default is Undefined, optional
        logging config
    properties : {str:str}, default is Undefined, optional
        A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
    queryFileUri : str, default is Undefined, optional
        The HCFS URI of the script that contains SQL queries.
        Conflicts with query_list
    queryList : [str], default is Undefined, optional
        The list of SQL queries or statements to execute as part of the job.
        Conflicts with query_file_uri
    scriptVariables : {str:str}, default is Undefined, optional
        Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
    """


    jarFileUris?: [str]

    loggingConfig?: [DataprocGcpUpboundIoV1beta1JobSpecInitProviderSparksqlConfigItems0LoggingConfigItems0]

    properties?: {str:str}

    queryFileUri?: str

    queryList?: [str]

    scriptVariables?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobSpecInitProviderSparksqlConfigItems0LoggingConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job spec init provider sparksql config items0 logging config items0

    Attributes
    ----------
    driverLogLevels : {str:str}, default is Undefined, optional
        The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    """


    driverLogLevels?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobSpecProviderConfigRef:
    """
    ProviderConfigReference specifies how the provider that will be used to
    create, observe, update, and delete this managed resource should be
    configured.

    Attributes
    ----------
    name : str, default is Undefined, required
        Name of the referenced object.
    policy : DataprocGcpUpboundIoV1beta1JobSpecProviderConfigRefPolicy, default is Undefined, optional
        policy
    """


    name: str

    policy?: DataprocGcpUpboundIoV1beta1JobSpecProviderConfigRefPolicy


schema DataprocGcpUpboundIoV1beta1JobSpecProviderConfigRefPolicy:
    """
    Policies for referencing.

    Attributes
    ----------
    resolution : str, default is "Required", optional
        Resolution specifies whether resolution of this reference is required.
        The default is 'Required', which means the reconcile will fail if the
        reference cannot be resolved. 'Optional' means this reference will be
        a no-op if it cannot be resolved.
    resolve : str, default is Undefined, optional
        Resolve specifies when this reference should be resolved. The default
        is 'IfNotPresent', which will attempt to resolve the reference only when
        the corresponding field is not present. Use 'Always' to resolve the
        reference on every reconcile.
    """


    resolution?: "Required" | "Optional" = "Required"

    resolve?: "Always" | "IfNotPresent"


schema DataprocGcpUpboundIoV1beta1JobSpecPublishConnectionDetailsTo:
    """
    PublishConnectionDetailsTo specifies the connection secret config which
    contains a name, metadata and a reference to secret store config to
    which any connection details for this managed resource should be written.
    Connection details frequently include the endpoint, username,
    and password required to connect to the managed resource.

    Attributes
    ----------
    configRef : DataprocGcpUpboundIoV1beta1JobSpecPublishConnectionDetailsToConfigRef, default is Undefined, optional
        config ref
    metadata : DataprocGcpUpboundIoV1beta1JobSpecPublishConnectionDetailsToMetadata, default is Undefined, optional
        metadata
    name : str, default is Undefined, required
        Name is the name of the connection secret.
    """


    configRef?: DataprocGcpUpboundIoV1beta1JobSpecPublishConnectionDetailsToConfigRef

    metadata?: DataprocGcpUpboundIoV1beta1JobSpecPublishConnectionDetailsToMetadata

    name: str


schema DataprocGcpUpboundIoV1beta1JobSpecPublishConnectionDetailsToConfigRef:
    """
    SecretStoreConfigRef specifies which secret store config should be used
    for this ConnectionSecret.

    Attributes
    ----------
    name : str, default is Undefined, required
        Name of the referenced object.
    policy : DataprocGcpUpboundIoV1beta1JobSpecPublishConnectionDetailsToConfigRefPolicy, default is Undefined, optional
        policy
    """


    name: str

    policy?: DataprocGcpUpboundIoV1beta1JobSpecPublishConnectionDetailsToConfigRefPolicy


schema DataprocGcpUpboundIoV1beta1JobSpecPublishConnectionDetailsToConfigRefPolicy:
    """
    Policies for referencing.

    Attributes
    ----------
    resolution : str, default is "Required", optional
        Resolution specifies whether resolution of this reference is required.
        The default is 'Required', which means the reconcile will fail if the
        reference cannot be resolved. 'Optional' means this reference will be
        a no-op if it cannot be resolved.
    resolve : str, default is Undefined, optional
        Resolve specifies when this reference should be resolved. The default
        is 'IfNotPresent', which will attempt to resolve the reference only when
        the corresponding field is not present. Use 'Always' to resolve the
        reference on every reconcile.
    """


    resolution?: "Required" | "Optional" = "Required"

    resolve?: "Always" | "IfNotPresent"


schema DataprocGcpUpboundIoV1beta1JobSpecPublishConnectionDetailsToMetadata:
    """
    Metadata is the metadata for connection secret.

    Attributes
    ----------
    annotations : {str:str}, default is Undefined, optional
        Annotations are the annotations to be added to connection secret.
        - For Kubernetes secrets, this will be used as "metadata.annotations".
        - It is up to Secret Store implementation for others store types.
    labels : {str:str}, default is Undefined, optional
        Labels are the labels/tags to be added to connection secret.
        - For Kubernetes secrets, this will be used as "metadata.labels".
        - It is up to Secret Store implementation for others store types.
    $type : str, default is Undefined, optional
        Type is the SecretType for the connection secret.
        - Only valid for Kubernetes Secret Stores.
    """


    annotations?: {str:str}

    labels?: {str:str}

    $type?: str


schema DataprocGcpUpboundIoV1beta1JobSpecWriteConnectionSecretToRef:
    """
    WriteConnectionSecretToReference specifies the namespace and name of a
    Secret to which any connection details for this managed resource should
    be written. Connection details frequently include the endpoint, username,
    and password required to connect to the managed resource.
    This field is planned to be replaced in a future release in favor of
    PublishConnectionDetailsTo. Currently, both could be set independently
    and connection details would be published to both without affecting
    each other.

    Attributes
    ----------
    name : str, default is Undefined, required
        Name of the secret.
    namespace : str, default is Undefined, required
        Namespace of the secret.
    """


    name: str

    namespace: str


schema DataprocGcpUpboundIoV1beta1JobStatus:
    """
    JobStatus defines the observed state of Job.

    Attributes
    ----------
    atProvider : DataprocGcpUpboundIoV1beta1JobStatusAtProvider, default is Undefined, optional
        at provider
    conditions : [DataprocGcpUpboundIoV1beta1JobStatusConditionsItems0], default is Undefined, optional
        Conditions of the resource.
    """


    atProvider?: DataprocGcpUpboundIoV1beta1JobStatusAtProvider

    conditions?: [DataprocGcpUpboundIoV1beta1JobStatusConditionsItems0]


schema DataprocGcpUpboundIoV1beta1JobStatusAtProvider:
    """
    dataproc gcp upbound io v1beta1 job status at provider

    Attributes
    ----------
    driverControlsFilesUri : str, default is Undefined, optional
        If present, the location of miscellaneous control files which may be used as part of job setup and handling. If not present, control files may be placed in the same location as driver_output_uri.
    driverOutputResourceUri : str, default is Undefined, optional
        A URI pointing to the location of the stdout of the job's driver program.
    effectiveLabels : {str:str}, default is Undefined, optional
        effective labels
    forceDelete : bool, default is Undefined, optional
        By default, you can only delete inactive jobs within
        Dataproc. Setting this to true, and calling destroy, will ensure that the
        job is first cancelled before issuing the delete.
    hadoopConfig : [DataprocGcpUpboundIoV1beta1JobStatusAtProviderHadoopConfigItems0], default is Undefined, optional
        hadoop config
    hiveConfig : [DataprocGcpUpboundIoV1beta1JobStatusAtProviderHiveConfigItems0], default is Undefined, optional
        hive config
    id : str, default is Undefined, optional
        id
    labels : {str:str}, default is Undefined, optional
        The list of labels (key/value pairs) to add to the job.
        Note: This field is non-authoritative, and will only manage the labels present in your configuration.
        Please refer to the field 'effective_labels' for all of the labels present on the resource.
    pigConfig : [DataprocGcpUpboundIoV1beta1JobStatusAtProviderPigConfigItems0], default is Undefined, optional
        pig config
    placement : [DataprocGcpUpboundIoV1beta1JobStatusAtProviderPlacementItems0], default is Undefined, optional
        placement
    prestoConfig : [DataprocGcpUpboundIoV1beta1JobStatusAtProviderPrestoConfigItems0], default is Undefined, optional
        presto config
    project : str, default is Undefined, optional
        The project in which the cluster can be found and jobs
        subsequently run against. If it is not provided, the provider project is used.
    pysparkConfig : [DataprocGcpUpboundIoV1beta1JobStatusAtProviderPysparkConfigItems0], default is Undefined, optional
        pyspark config
    reference : [DataprocGcpUpboundIoV1beta1JobStatusAtProviderReferenceItems0], default is Undefined, optional
        reference
    region : str, default is Undefined, optional
        The Cloud Dataproc region. This essentially determines which clusters are available
        for this job to be submitted to. If not specified, defaults to global.
    scheduling : [DataprocGcpUpboundIoV1beta1JobStatusAtProviderSchedulingItems0], default is Undefined, optional
        scheduling
    sparkConfig : [DataprocGcpUpboundIoV1beta1JobStatusAtProviderSparkConfigItems0], default is Undefined, optional
        spark config
    sparksqlConfig : [DataprocGcpUpboundIoV1beta1JobStatusAtProviderSparksqlConfigItems0], default is Undefined, optional
        sparksql config
    status : [DataprocGcpUpboundIoV1beta1JobStatusAtProviderStatusItems0], default is Undefined, optional
        status
    terraformLabels : {str:str}, default is Undefined, optional
        The combination of labels configured directly on the resource and default labels configured on the provider.
    """


    driverControlsFilesUri?: str

    driverOutputResourceUri?: str

    effectiveLabels?: {str:str}

    forceDelete?: bool

    hadoopConfig?: [DataprocGcpUpboundIoV1beta1JobStatusAtProviderHadoopConfigItems0]

    hiveConfig?: [DataprocGcpUpboundIoV1beta1JobStatusAtProviderHiveConfigItems0]

    id?: str

    labels?: {str:str}

    pigConfig?: [DataprocGcpUpboundIoV1beta1JobStatusAtProviderPigConfigItems0]

    placement?: [DataprocGcpUpboundIoV1beta1JobStatusAtProviderPlacementItems0]

    prestoConfig?: [DataprocGcpUpboundIoV1beta1JobStatusAtProviderPrestoConfigItems0]

    project?: str

    pysparkConfig?: [DataprocGcpUpboundIoV1beta1JobStatusAtProviderPysparkConfigItems0]

    reference?: [DataprocGcpUpboundIoV1beta1JobStatusAtProviderReferenceItems0]

    region?: str

    scheduling?: [DataprocGcpUpboundIoV1beta1JobStatusAtProviderSchedulingItems0]

    sparkConfig?: [DataprocGcpUpboundIoV1beta1JobStatusAtProviderSparkConfigItems0]

    sparksqlConfig?: [DataprocGcpUpboundIoV1beta1JobStatusAtProviderSparksqlConfigItems0]

    status?: [DataprocGcpUpboundIoV1beta1JobStatusAtProviderStatusItems0]

    terraformLabels?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobStatusAtProviderHadoopConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job status at provider hadoop config items0

    Attributes
    ----------
    archiveUris : [str], default is Undefined, optional
        HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
    args : [str], default is Undefined, optional
        The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
    fileUris : [str], default is Undefined, optional
        HCFS URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
    jarFileUris : [str], default is Undefined, optional
        HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
    loggingConfig : [DataprocGcpUpboundIoV1beta1JobStatusAtProviderHadoopConfigItems0LoggingConfigItems0], default is Undefined, optional
        logging config
    mainClass : str, default is Undefined, optional
        The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in jar_file_uris. Conflicts with main_jar_file_uri
    mainJarFileUri : str, default is Undefined, optional
        The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'. Conflicts with main_class
    properties : {str:str}, default is Undefined, optional
        A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code..
    """


    archiveUris?: [str]

    args?: [str]

    fileUris?: [str]

    jarFileUris?: [str]

    loggingConfig?: [DataprocGcpUpboundIoV1beta1JobStatusAtProviderHadoopConfigItems0LoggingConfigItems0]

    mainClass?: str

    mainJarFileUri?: str

    properties?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobStatusAtProviderHadoopConfigItems0LoggingConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job status at provider hadoop config items0 logging config items0

    Attributes
    ----------
    driverLogLevels : {str:str}, default is Undefined, optional
        The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    """


    driverLogLevels?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobStatusAtProviderHiveConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job status at provider hive config items0

    Attributes
    ----------
    continueOnFailure : bool, default is Undefined, optional
        Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
    jarFileUris : [str], default is Undefined, optional
        HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
    properties : {str:str}, default is Undefined, optional
        A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code..
    queryFileUri : str, default is Undefined, optional
        HCFS URI of file containing Hive script to execute as the job.
        Conflicts with query_list
    queryList : [str], default is Undefined, optional
        The list of Hive queries or statements to execute as part of the job.
        Conflicts with query_file_uri
    scriptVariables : {str:str}, default is Undefined, optional
        Mapping of query variable names to values (equivalent to the Hive command: SET name="value";).
    """


    continueOnFailure?: bool

    jarFileUris?: [str]

    properties?: {str:str}

    queryFileUri?: str

    queryList?: [str]

    scriptVariables?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobStatusAtProviderPigConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job status at provider pig config items0

    Attributes
    ----------
    continueOnFailure : bool, default is Undefined, optional
        Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
    jarFileUris : [str], default is Undefined, optional
        HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
    loggingConfig : [DataprocGcpUpboundIoV1beta1JobStatusAtProviderPigConfigItems0LoggingConfigItems0], default is Undefined, optional
        logging config
    properties : {str:str}, default is Undefined, optional
        A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
    queryFileUri : str, default is Undefined, optional
        HCFS URI of file containing Hive script to execute as the job.
        Conflicts with query_list
    queryList : [str], default is Undefined, optional
        The list of Hive queries or statements to execute as part of the job.
        Conflicts with query_file_uri
    scriptVariables : {str:str}, default is Undefined, optional
        Mapping of query variable names to values (equivalent to the Pig command: name=[value]).
    """


    continueOnFailure?: bool

    jarFileUris?: [str]

    loggingConfig?: [DataprocGcpUpboundIoV1beta1JobStatusAtProviderPigConfigItems0LoggingConfigItems0]

    properties?: {str:str}

    queryFileUri?: str

    queryList?: [str]

    scriptVariables?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobStatusAtProviderPigConfigItems0LoggingConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job status at provider pig config items0 logging config items0

    Attributes
    ----------
    driverLogLevels : {str:str}, default is Undefined, optional
        The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    """


    driverLogLevels?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobStatusAtProviderPlacementItems0:
    """
    dataproc gcp upbound io v1beta1 job status at provider placement items0

    Attributes
    ----------
    clusterName : str, default is Undefined, optional
        The name of the cluster where the job
        will be submitted.
    clusterUuid : str, default is Undefined, optional
        A cluster UUID generated by the Cloud Dataproc service when the job is submitted.
    """


    clusterName?: str

    clusterUuid?: str


schema DataprocGcpUpboundIoV1beta1JobStatusAtProviderPrestoConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job status at provider presto config items0

    Attributes
    ----------
    clientTags : [str], default is Undefined, optional
        Presto client tags to attach to this query.
    continueOnFailure : bool, default is Undefined, optional
        Whether to continue executing queries if a query fails. Setting to true can be useful when executing independent parallel queries. Defaults to false.
    loggingConfig : [DataprocGcpUpboundIoV1beta1JobStatusAtProviderPrestoConfigItems0LoggingConfigItems0], default is Undefined, optional
        logging config
    outputFormat : str, default is Undefined, optional
        The format in which query output will be displayed. See the Presto documentation for supported output formats.
    properties : {str:str}, default is Undefined, optional
        A mapping of property names to values. Used to set Presto session properties Equivalent to using the --session flag in the Presto CLI.
    queryFileUri : str, default is Undefined, optional
        The HCFS URI of the script that contains SQL queries.
        Conflicts with query_list
    queryList : [str], default is Undefined, optional
        The list of SQL queries or statements to execute as part of the job.
        Conflicts with query_file_uri
    """


    clientTags?: [str]

    continueOnFailure?: bool

    loggingConfig?: [DataprocGcpUpboundIoV1beta1JobStatusAtProviderPrestoConfigItems0LoggingConfigItems0]

    outputFormat?: str

    properties?: {str:str}

    queryFileUri?: str

    queryList?: [str]


schema DataprocGcpUpboundIoV1beta1JobStatusAtProviderPrestoConfigItems0LoggingConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job status at provider presto config items0 logging config items0

    Attributes
    ----------
    driverLogLevels : {str:str}, default is Undefined, optional
        The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    """


    driverLogLevels?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobStatusAtProviderPysparkConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job status at provider pyspark config items0

    Attributes
    ----------
    archiveUris : [str], default is Undefined, optional
        HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
    args : [str], default is Undefined, optional
        The arguments to pass to the driver.
    fileUris : [str], default is Undefined, optional
        HCFS URIs of files to be copied to the working directory of Python drivers and distributed tasks. Useful for naively parallel tasks.
    jarFileUris : [str], default is Undefined, optional
        HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
    loggingConfig : [DataprocGcpUpboundIoV1beta1JobStatusAtProviderPysparkConfigItems0LoggingConfigItems0], default is Undefined, optional
        logging config
    mainPythonFileUri : str, default is Undefined, optional
        The HCFS URI of the main Python file to use as the driver. Must be a .py file.
    properties : {str:str}, default is Undefined, optional
        A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
    pythonFileUris : [str], default is Undefined, optional
        HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
    """


    archiveUris?: [str]

    args?: [str]

    fileUris?: [str]

    jarFileUris?: [str]

    loggingConfig?: [DataprocGcpUpboundIoV1beta1JobStatusAtProviderPysparkConfigItems0LoggingConfigItems0]

    mainPythonFileUri?: str

    properties?: {str:str}

    pythonFileUris?: [str]


schema DataprocGcpUpboundIoV1beta1JobStatusAtProviderPysparkConfigItems0LoggingConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job status at provider pyspark config items0 logging config items0

    Attributes
    ----------
    driverLogLevels : {str:str}, default is Undefined, optional
        The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    """


    driverLogLevels?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobStatusAtProviderReferenceItems0:
    """
    dataproc gcp upbound io v1beta1 job status at provider reference items0

    Attributes
    ----------
    jobId : str, default is Undefined, optional
        job Id
    """


    jobId?: str


schema DataprocGcpUpboundIoV1beta1JobStatusAtProviderSchedulingItems0:
    """
    dataproc gcp upbound io v1beta1 job status at provider scheduling items0

    Attributes
    ----------
    maxFailuresPerHour : float, default is Undefined, optional
        Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
    maxFailuresTotal : float, default is Undefined, optional
        Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
    """


    maxFailuresPerHour?: float

    maxFailuresTotal?: float


schema DataprocGcpUpboundIoV1beta1JobStatusAtProviderSparkConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job status at provider spark config items0

    Attributes
    ----------
    archiveUris : [str], default is Undefined, optional
        HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
    args : [str], default is Undefined, optional
        The arguments to pass to the driver.
    fileUris : [str], default is Undefined, optional
        HCFS URIs of files to be copied to the working directory of Spark drivers and distributed tasks. Useful for naively parallel tasks.
    jarFileUris : [str], default is Undefined, optional
        HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
    loggingConfig : [DataprocGcpUpboundIoV1beta1JobStatusAtProviderSparkConfigItems0LoggingConfigItems0], default is Undefined, optional
        logging config
    mainClass : str, default is Undefined, optional
        The class containing the main method of the driver. Must be in a
        provided jar or jar that is already on the classpath. Conflicts with main_jar_file_uri
    mainJarFileUri : str, default is Undefined, optional
        The HCFS URI of jar file containing
        the driver jar. Conflicts with main_class
    properties : {str:str}, default is Undefined, optional
        A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
    """


    archiveUris?: [str]

    args?: [str]

    fileUris?: [str]

    jarFileUris?: [str]

    loggingConfig?: [DataprocGcpUpboundIoV1beta1JobStatusAtProviderSparkConfigItems0LoggingConfigItems0]

    mainClass?: str

    mainJarFileUri?: str

    properties?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobStatusAtProviderSparkConfigItems0LoggingConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job status at provider spark config items0 logging config items0

    Attributes
    ----------
    driverLogLevels : {str:str}, default is Undefined, optional
        The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    """


    driverLogLevels?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobStatusAtProviderSparksqlConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job status at provider sparksql config items0

    Attributes
    ----------
    jarFileUris : [str], default is Undefined, optional
        HCFS URIs of jar files to be added to the Spark CLASSPATH.
    loggingConfig : [DataprocGcpUpboundIoV1beta1JobStatusAtProviderSparksqlConfigItems0LoggingConfigItems0], default is Undefined, optional
        logging config
    properties : {str:str}, default is Undefined, optional
        A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
    queryFileUri : str, default is Undefined, optional
        The HCFS URI of the script that contains SQL queries.
        Conflicts with query_list
    queryList : [str], default is Undefined, optional
        The list of SQL queries or statements to execute as part of the job.
        Conflicts with query_file_uri
    scriptVariables : {str:str}, default is Undefined, optional
        Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
    """


    jarFileUris?: [str]

    loggingConfig?: [DataprocGcpUpboundIoV1beta1JobStatusAtProviderSparksqlConfigItems0LoggingConfigItems0]

    properties?: {str:str}

    queryFileUri?: str

    queryList?: [str]

    scriptVariables?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobStatusAtProviderSparksqlConfigItems0LoggingConfigItems0:
    """
    dataproc gcp upbound io v1beta1 job status at provider sparksql config items0 logging config items0

    Attributes
    ----------
    driverLogLevels : {str:str}, default is Undefined, optional
        The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
    """


    driverLogLevels?: {str:str}


schema DataprocGcpUpboundIoV1beta1JobStatusAtProviderStatusItems0:
    """
    dataproc gcp upbound io v1beta1 job status at provider status items0

    Attributes
    ----------
    details : str, default is Undefined, optional
        Optional job state details, such as an error description if the state is ERROR.
    state : str, default is Undefined, optional
        A state message specifying the overall job state.
    stateStartTime : str, default is Undefined, optional
        The time when this state was entered.
    substate : str, default is Undefined, optional
        Additional state information, which includes status reported by the agent.
    """


    details?: str

    state?: str

    stateStartTime?: str

    substate?: str


schema DataprocGcpUpboundIoV1beta1JobStatusConditionsItems0:
    """
    A Condition that may apply to a resource.

    Attributes
    ----------
    lastTransitionTime : str, default is Undefined, required
        LastTransitionTime is the last time this condition transitioned from one
        status to another.
    message : str, default is Undefined, optional
        A Message containing details about this condition's last transition from
        one status to another, if any.
    reason : str, default is Undefined, required
        A Reason for this condition's last transition from one status to another.
    status : str, default is Undefined, required
        Status of this condition; is it currently True, False, or Unknown?
    $type : str, default is Undefined, required
        Type of this condition. At most one of each condition type may apply to
        a resource at any point in time.
    """


    lastTransitionTime: str

    message?: str

    reason: str

    status: str

    $type: str


